# Cache-Augmented Generation (CAG) ‚Äì JavaScript Teaching Repository

Welcome!  This mini-repository is intentionally **didactic**: every file is a self-contained lesson that incrementally introduces the idea of _Cache-Augmented Generation_ (CAG) ‚Äì a technique in which we preload relevant documents into an LLM‚Äôs context so the model can answer many questions **without performing a live retrieval step**.

The code is kept short, dependency-free and thoroughly logged so that you can run each script with plain Node and _see_ what is happening at every phase.

```bash
# run any demo
node cag_demo.js
```

---

## File Tour

| File | Pedagogical Focus | Key Concepts Introduced |
|------|-------------------|-------------------------|
| **cag_demo.js** | üçè _‚ÄúHello CAG‚Äù_ ‚Äì the smallest viable example | ‚Ä¢ Pre-loaded text cache<br>‚Ä¢ Linear string matching |
| **cag_demo_with_vector_store.js** | üçä Adds semantic search via fake embeddings | ‚Ä¢ Document vectors<br>‚Ä¢ Cosine similarity<br>‚Ä¢ Similarity threshold (`0.85` default) |
| **cache_augmented_llm.js** | üçé Modularises the code into a reusable `CacheAugmentedLLM` class and layers extra features | ‚Ä¢ Embedding-function injection<br>‚Ä¢ Vector-store plug-in stub<br>‚Ä¢ Query-result cache (performance)<br>‚Ä¢ Runtime similarity-threshold tuning |
| **cache_augmented_llm_with_search.js** | üçâ Separates the pipeline even further to highlight each sub-step | ‚Ä¢ Dedicated helpers: `vectorizeQuery` & `searchVectorStore`<br>‚Ä¢ Clear trace of _vectorise ‚ûú search ‚ûú answer_ |

> **Tip for educators** ‚Äì Because each successive file only adds a single conceptual leap, you can walk learners through the scripts one after another, live-coding small deltas or using `git diff` to highlight the change.

---

## Suggested Learning Path

1. **Run `cag_demo.js`** to observe basic string matching and discuss its limitations (lexical vs semantic).
2. **Move to `cag_demo_with_vector_store.js`** to show how embeddings plus cosine similarity overcome those limits.
3. **Graduate to `cache_augmented_llm.js`** for a conversation about real-world concerns: external vector stores, plug-able embeddings and caching for latency.
4. **Finish with `cache_augmented_llm_with_search.js`** to underline the standard retrieval pipeline that underpins most production systems.

---

## Why CAG instead of RAG?

Retrieval-Augmented Generation (RAG) fetches documents _at query time_. CAG pre-loads a carefully selected subset into the model‚Äôs context (or fast in-memory vector store), trading memory for speed.  This repo lets students experiment with that trade-off before touching heavyweight libraries or cloud services.

---

## Running the demos

All scripts rely only on the Node.js standard library.

```bash
# Execute a script
node cache_augmented_llm.js

# View verbose logs for learning
NODE_OPTIONS="--trace-warnings" node cache_augmented_llm_with_search.js
```

Feel free to modify the **`contextCache`** objects, tweak the **`similarityThreshold`**, or replace the fake embedding function with your own model to explore further.

---

Happy learning ‚Äì and happy caching! üéâ

---

## Repository owner

This repository and all provided assets are maintained by **admin@nguyenhongquan.com**.

